<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最新提交：ChatGPT</title>
    <link>https://www.reddit.com/r/ChatGPT/new</link>
    <description>讨论 ChatGPT 和 AI 的子版块。与 OpenAI 无关。谢谢，Nat！</description>
    <lastBuildDate>Thu, 23 Jan 2025 09:23:32 GMT</lastBuildDate>
    <item>
      <title>您认为这些模型的 OpenAI 对应物是什么？</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7zh9n/what_do_you_think_are_openai_counterparts_of/</link>
      <description><![CDATA[        提交人    /u/lelouchlamperouge52   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7zh9n/what_do_you_think_are_openai_counterparts_of/</guid>
      <pubDate>Thu, 23 Jan 2025 09:15:01 GMT</pubDate>
    </item>
    <item>
      <title>AI无法区分足球和美式足球XD</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7zg1x/ai_cannot_differentiate_between_football_and/</link>
      <description><![CDATA[        提交人    /u/Consistent-Ad417   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7zg1x/ai_cannot_differentiate_between_football_and/</guid>
      <pubDate>Thu, 23 Jan 2025 09:12:11 GMT</pubDate>
    </item>
    <item>
      <title>史上最大的伪君子——萨姆·奥尔特曼</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7zeik/the_biggest_hypocrite_to_ever_exist_sam_altman/</link>
      <description><![CDATA[      萨姆·奥特曼在就职典礼结束后就改变了对唐纳德的看法，这真是太正直了。萨姆·奥特曼真是一个有道德的人！   由    /u/No_Macaroon_7608  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7zeik/the_biggest_hypocrite_to_ever_exist_sam_altman/</guid>
      <pubDate>Thu, 23 Jan 2025 09:08:38 GMT</pubDate>
    </item>
    <item>
      <title>这就是我在代理和 LLM 应用程序中使用推理模型 (Deepseek R1) 的方式</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7zdkc/this_is_how_i_use_reasoning_models_deepseek_r1_in/</link>
      <description><![CDATA[    /u/jasonzhou1993   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7zdkc/this_is_how_i_use_reasoning_models_deepseek_r1_in/</guid>
      <pubDate>Thu, 23 Jan 2025 09:06:28 GMT</pubDate>
    </item>
    <item>
      <title>一个有用的守门人，保护着羊群。</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7z6fo/a_useful_gatekeeper_protecting_the_flock/</link>
      <description><![CDATA[如何通过会话捕获“和我想法一样”的人？想象一个守门应用程序，它通过简短的互动会话消除巨魔，以嗅出装腔作势的人。适用于任何事物，政治、宗教、社会团体、地点或粉丝圈。如何诱捕试图通过大门的装腔作势者和巨魔，同时让真正的演员通过。矛盾的是，如何通过问一些不可搜索的问题来诱骗人们暴露自己，而这些问题只会对装腔作势者来说是违反直觉的！    提交人    /u/PRATYEKABUDDHAYANA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7z6fo/a_useful_gatekeeper_protecting_the_flock/</guid>
      <pubDate>Thu, 23 Jan 2025 08:50:52 GMT</pubDate>
    </item>
    <item>
      <title>为 LLM 开发潜在空间神经模块</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7z2o5/develop_latent_space_neural_module_for_llm/</link>
      <description><![CDATA[为了开发用于大型语言模型 (LLM) 的潜在空间神经模块，我们可以遵循这些原则和步骤，这些原则和步骤受到记忆和神经架构领域最先进研究的启发，但不包括对 Titans 论文本身的引用：  核心概念基础  LLM 的潜在空间神经模块应该模拟人类记忆的关键方面：• 短期记忆：用于维持即时上下文，类似于 Transformers 中的注意力机制。• 长期记忆：用于有效地编码、压缩和检索历史数据，同时扩展到长上下文。 • 潜在空间动力学：利用深度非线性架构将输入标记投射到内存丰富的潜在表示中，提高效率和泛化能力。  记忆模块设计  深度潜在记忆模块应解决关键挑战： 编码历史背景 • 使用参数化的记忆模块 M_t，该模块能够学习历史抽象并在深度潜在空间中表示压缩信息。 • 集成以下机制： • 基于意外的更新：使用基于梯度的意外指标对意外或新信息进行优先排序。 • 自适应遗忘：采用门控机制自适应地删除不相关信息，避免内存溢出。 建议的内存更新机制 Mt = (1 - \alpha_t)M{t-1} + \etat S{t-1} - \thetat \nabla \ell(M{t-1}; x_t) 其中： • \alpha_t：遗忘门（根据上下文变化自适应）。 • \eta_t：过去意外的动量。 • \theta_t：调节新信息的影响。 深度非线性记忆 结合多层神经网络 f(M_t) 以实现： • 丰富的表示：捕获数据序列中的复杂依赖关系。 • 扩展效率：通过张量化操作（例如矩阵乘法）确保高效训练。  集成到 LLM 架构中  将记忆模块合并到 LLM 架构中的三种可能方法： (a) 记忆作为上下文 (MAC) • 将短期（基于注意力）和长期记忆结合作为上下文输入。 • 将检索到的记忆与当前上下文一起使用： y_t = \text{Attention}([p, M_t, x_t]) • p：持久的与任务无关的记忆。 • M_t：从历史序列中检索到的记忆。  (b) 记忆作为门控分支 (MAG) • 具有两个分支的并行处理：1. 基于注意力的短期记忆。2. 长期记忆模块 M(x)。 • 通过门控机制组合输出： o_t = g(\text{Attention}(x_t), M(x_t)) • g：门控函数，用于平衡短期和长期记忆的贡献。  (c) 记忆即层 (MAL) • 将记忆模块视为 LLM 堆栈中的中间层。 • 在将过去的信息传递到下游层之前对其进行压缩。  并行化和训练 • 张量化更新：使用分块优化和矩阵运算实现记忆模块的有效并行训练。 • 数据相关参数：允许关键超参数（例如 \alpha_t、\eta_t）依赖于输入序列或块，以实现更好的适应性。 评估和优化 • 可扩展性：在长上下文任务（如时间序列预测、语言建模和 NIAH 风格的任务）上进行测试。 • 微调：使用元学习技术优化记忆模块参数，以跨域推广。 • 深度分析：研究记忆深度对下游任务性能的影响。  这个潜在空间神经模块使 LLM 能够有效地处理长上下文，利用先进的记忆和神经机制。您想探索任何特定组件的实现细节吗？    提交人    /u/Worldly_Evidence9113   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7z2o5/develop_latent_space_neural_module_for_llm/</guid>
      <pubDate>Thu, 23 Jan 2025 08:42:47 GMT</pubDate>
    </item>
    <item>
      <title>“思考”之类的东西只是为了节省处理能力的骗局。</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yzew/the_thinking_etc_thing_is_just_a_scam_to_save_on/</link>
      <description><![CDATA[它真的什么都不做，很烦人。GPT 过去只是立即提供答案，现在它被编程为像人类一样行事……我讨厌它。    由   提交  /u/Pantim   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yzew/the_thinking_etc_thing_is_just_a_scam_to_save_on/</guid>
      <pubDate>Thu, 23 Jan 2025 08:35:41 GMT</pubDate>
    </item>
    <item>
      <title>肖恩·吉利斯（Shane Gillis）在《染色体》中</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yyqg/shane_gillis_in_the_chromosome/</link>
      <description><![CDATA[        由    /u/dingodangomango  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yyqg/shane_gillis_in_the_chromosome/</guid>
      <pubDate>Thu, 23 Jan 2025 08:34:17 GMT</pubDate>
    </item>
    <item>
      <title>我做了一件让自己开心的事</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yy3l/i_made_something_im_happy_with/</link>
      <description><![CDATA[      我涉足视频领域，对此我感到非常高兴。我是这方面的新手，经过一番努力，我对这个短篇故事非常满意。AI Mad World    提交人    /u/the_pirate_roberts   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yy3l/i_made_something_im_happy_with/</guid>
      <pubDate>Thu, 23 Jan 2025 08:32:54 GMT</pubDate>
    </item>
    <item>
      <title>街头霸王致敬 | Hadou-Phoria | Kling & Udio</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7ywnv/street_fighter_tribute_hadouphoria_kling_udio/</link>
      <description><![CDATA[  由    /u/TradingCardGirl  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7ywnv/street_fighter_tribute_hadouphoria_kling_udio/</guid>
      <pubDate>Thu, 23 Jan 2025 08:29:53 GMT</pubDate>
    </item>
    <item>
      <title>deepseek 是一个附带项目 - openai 需要提高他们的水平</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yv7q/deepseek_is_a_side_project_openai_needs_to_up/</link>
      <description><![CDATA[        提交人    /u/eternviking   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yv7q/deepseek_is_a_side_project_openai_needs_to_up/</guid>
      <pubDate>Thu, 23 Jan 2025 08:26:45 GMT</pubDate>
    </item>
    <item>
      <title>微笑带来多大的变化</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yude/what_a_difference_a_smile_makes/</link>
      <description><![CDATA[        提交人    /u/Internet--Traveller   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yude/what_a_difference_a_smile_makes/</guid>
      <pubDate>Thu, 23 Jan 2025 08:25:02 GMT</pubDate>
    </item>
    <item>
      <title>数学不是数学</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7ysla/maths_aint_mathing/</link>
      <description><![CDATA[        提交人    /u/Advanced_General6524   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7ysla/maths_aint_mathing/</guid>
      <pubDate>Thu, 23 Jan 2025 08:21:06 GMT</pubDate>
    </item>
    <item>
      <title>这个 AI 机器人是不是太真实了？🤖😱</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yqol/is_this_ai_robot_too_real/</link>
      <description><![CDATA[       由    /u/dotcomgeek  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yqol/is_this_ai_robot_too_real/</guid>
      <pubDate>Thu, 23 Jan 2025 08:16:57 GMT</pubDate>
    </item>
    <item>
      <title>事实上，Copilot 365 太糟糕了，所以没有被列入基准测试吗？</title>
      <link>https://www.reddit.com/r/ChatGPT/comments/1i7yqap/is_the_fact_copilot_365_is_so_bad_the_reason_for/</link>
      <description><![CDATA[我的公司正在评估采用能够协助所有员工的 AI 的选项。 而那些该死的傻瓜认为最好的选择是 Copilot 365 而不是 4o / Sonnet / DeepSeek R1 / Gemini 因此，如果您对两个问题有见解，我将不胜感激： 1) Copilot 背后的 LLM 目前是否真的可能未包含在任何基准中？（经过一番研究似乎确实如此）。如果是，为什么？因为它真的很糟糕？还是出于其他技术原因？ 2) 我对 Copilot 365 的强烈负面看法（考虑到所有替代方案）是否有偏见？    提交人    /u/You_Read_That   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ChatGPT/comments/1i7yqap/is_the_fact_copilot_365_is_so_bad_the_reason_for/</guid>
      <pubDate>Thu, 23 Jan 2025 08:16:06 GMT</pubDate>
    </item>
    </channel>
</rss>