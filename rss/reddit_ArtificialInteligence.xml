<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能网关</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>r/ArtificialIntelligence 的目标是为人工智能社区的方方面面提供门户，并促进有关我们所知的人工智能理念和概念的讨论。这些可能包括哲学和社会问题、艺术和设计、技术论文、机器学习、在哪里可以找到资源和工具、如何开发人工智能/机器学习项目、商业中的人工智能、人工智能如何影响我们的生活、未来可能的发展方向以及许多其他主题。欢迎。</description>
    <lastBuildDate>Wed, 16 Oct 2024 01:46:23 GMT</lastBuildDate>
    <item>
      <title>如何在家运行类似 NVLM-D-72B 的东西？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4lb3s/how_to_run_something_like_nvlmd72b_at_home/</link>
      <description><![CDATA[我有一台配备 RTX4090 的 PC，这是一款相当高端的消费级 NVIDIA 显卡……但它有 24GB 内存，根据这篇文章，这对于像这样的 72B 参数模型来说还不够。它建议了各种节省成本的方法，并让某些东西在性能降低或速度大幅降低的情况下运行。 但这让我想到：你会如何在家中运行这种规模的模型？需要什么？如果您要构建一台机器来运行此模型或类似大小的模型，或多或少实时地运行，您会在其中放入什么？ 只是想了解家庭用户是否有可能在本地运行这些前沿模型之一，或者我是否应该坚持使用大型 AI 公司的服务。（或者我猜想一个可能的中间立场 — 在云中的某个 VM 上运行我自己的模型，但在这里我再次想知道我需要寻找什么规格来运行这样的模型。）    提交人    /u/JoeStrout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4lb3s/how_to_run_something_like_nvlmd72b_at_home/</guid>
      <pubDate>Tue, 15 Oct 2024 23:16:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么 AI 会讨厌你？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4kl62/whats_with_the_ai_hate/</link>
      <description><![CDATA[大家好，我刚刚想到了一些（我当时想的）有趣的想法，想制作一些有趣的视频。由于我自己不会设计或制作动画，所以 AI 视频是获得结果的绝佳解决方案。完成后，我很高兴向朋友和网上展示它。使用 AI 遭到了很多批评。我对此感到非常惊讶，仍然不明白。  对于任何对我所说的内容感兴趣的人：https://youtu.be/M-Ndoco3QHQ?si=y5iGjOMcMF2KlBgR    提交人    /u/TheMythGuido   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4kl62/whats_with_the_ai_hate/</guid>
      <pubDate>Tue, 15 Oct 2024 22:42:53 GMT</pubDate>
    </item>
    <item>
      <title>HHEM（幻觉评估模型）下载量突破 200 万</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4k7wt/hhem_hallucination_evaluation_model_passes_2m/</link>
      <description><![CDATA[我很高兴地告诉大家，u/vectara 的 HHEM（又名幻觉评估模型）在 HuggingFace 上的下载量刚刚突破 200 万次。我们对加速采用感到兴奋，并致力于开发更多更好的方法来检测和消除幻觉，这是让 RAG 为企业服务的关键组成部分。    提交人    /u/ofermend   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4k7wt/hhem_hallucination_evaluation_model_passes_2m/</guid>
      <pubDate>Tue, 15 Oct 2024 22:25:42 GMT</pubDate>
    </item>
    <item>
      <title>请告诉我一个好的图像生成器</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4k781/please_tell_me_a_good_image_generator/</link>
      <description><![CDATA[不是 google gemini 我想要一个好的图像生成器来创建东西我想创建一个 slugcat 它们可爱的猫科动物所以请告诉我一些东西也不是超现实的东西并且可以从其他图像中获取风格    提交人    /u/IllustriousCase486   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4k781/please_tell_me_a_good_image_generator/</guid>
      <pubDate>Tue, 15 Oct 2024 22:24:49 GMT</pubDate>
    </item>
    <item>
      <title>我被要求在工作中举办一个研讨会，旨在鼓励整个企业的员工考虑在日常工作中使用人工智能。有什么想法可以让这个活动变得有趣吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4k6m8/i_have_been_asked_to_run_a_workshop_at_work_aimed/</link>
      <description><![CDATA[几个月前我在 ChatGPT 上运行过一个，引起了很多人的兴趣，他们希望我进行一次复习，帮助不同部门的人提出如何使用它来改进他们自己的流程，甚至改进我们的服务/产品的想法。  有没有什么活动想法或例子可以用来产生这种有机的灵感？    提交人    /u/i-am-a-passenger   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4k6m8/i_have_been_asked_to_run_a_workshop_at_work_aimed/</guid>
      <pubDate>Tue, 15 Oct 2024 22:24:03 GMT</pubDate>
    </item>
    <item>
      <title>人工智能处理的可视化</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4jfqd/visualization_of_ai_processing/</link>
      <description><![CDATA[嘿，我很好奇是否有任何动画可以直观地展示人工智能的工作原理。我现在正在使用笔记本 LM，并让它将讲座笔记传给我。在我的脑海里，我明白我只是提供了一个技术上的文件，但这个文件（视频）与它有实际的时间关联，我不明白人工智能如何能够绕过时间并在 1 秒内理解整个视频。 好奇你的想法或我如何才能更好地理解它是如何工作的。    提交人    /u/TheoDubsWashington   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4jfqd/visualization_of_ai_processing/</guid>
      <pubDate>Tue, 15 Oct 2024 21:49:57 GMT</pubDate>
    </item>
    <item>
      <title>Mindalogue LLM——为有效学习和任务探索提供动力的非线性交互</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4jbcc/mindalogue_llm_powered_nonlinear_interaction_for/</link>
      <description><![CDATA[标题：Mindalogue LLM——用于有效学习和任务探索的非线性交互 我每天都在寻找和总结有趣的人工智能研究论文，这样你就不必把它们全部翻遍了。今天的论文题为“Mindalogue：LLM——用于有效学习和任务探索的非线性交互”，作者是 Rui Zhang、Ziyao Zhang、Fengliang Zhu、Jiajie Zhou 和 Anyi Rao。 这项研究探讨了当前大型语言模型 (LLM) 的局限性，其线性交互方法在处理复杂任务时通常会增加用户的认知负荷和操作成本。为了应对这些挑战，作者提出了Mindalogue，这是一种非线性交互系统，旨在提高用户处理复杂信息的效率和适应性。 研究的主要发现包括：  非线性交互模型：与遵循逐步顺序交互模型的传统LLM不同，Mindalogue使用“节点+画布”方法，允许用户以非线性方式与信息交互。此功能通过启用灵活的探索路径，显着减少了认知负荷并增强了用户参与度。 提高任务理解力和效率：Mindalogue 的信息图表和图形表示可提供更清晰的逻辑结构，帮助用户更好地理解和组织复杂信息，从而提高任务性能和效率。 评估和比较优势：在一项有 16 名参与者的研究中，Mindalogue 在用户满意度、减少认知负荷和改进信息综合方面表现出优于传统线性系统的性能。该系统利用人工智能进行解释、示例生成和探索功能，实现与信息的多方面结构化互动。 可用性反馈：虽然 Mindalogue 提供了更大的探索性和灵活性，但与传统系统相比，学习曲线略有增加。然而，一旦熟悉了，用户就会从系统的操作自由中受益匪浅。 应用潜力：该研究表明，Mindalogue 在需要复杂信息处理的领域具有巨大潜在应用，例如教育、研究和跨学科任务管理，为非线性系统设计提供了见解，以提高生产率。  您可以在此处查看完整的细分：这里 您可以在此处查看完整的原始研究论文：原始论文    提交人    /u/steves1189   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4jbcc/mindalogue_llm_powered_nonlinear_interaction_for/</guid>
      <pubDate>Tue, 15 Oct 2024 21:44:29 GMT</pubDate>
    </item>
    <item>
      <title>刚刚发现人工智能被用于无人驾驶 F1 赛车——这太疯狂了</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4huhd/just_discovered_ai_is_being_used_for_driverless/</link>
      <description><![CDATA[我完全不知道还有自动驾驶赛车这种东西！看到这些方程式赛车在没有人类驾驶员的情况下如何进行高速决策并相互超车，真是令人着迷。我想知道他们可能会使用什么样的人工智能技术？强化学习在这方面会很有效。 有点想低调地观看 F1 Drive to Survive 的“极客”版本，看看他们是如何做到的。 https://youtu.be/Kgh5gMFhFYE?si=pIqoPc5ehiUWEKvi    提交人    /u/fluffpudding   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4huhd/just_discovered_ai_is_being_used_for_driverless/</guid>
      <pubDate>Tue, 15 Oct 2024 20:41:03 GMT</pubDate>
    </item>
    <item>
      <title>假人用人工智能</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4gm95/ai_for_dummy/</link>
      <description><![CDATA[大家好，我 40 多岁，对目前的工作感到厌倦，但发现 AI 很有趣。我对编码和 AI 一无所知。互联网上的信息让我应接不暇。有谁能分享一个路线图、播客、视频博客让我入门？我所拥有的只是学习的热情。谢谢！    提交人    /u/j0hnard133   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4gm95/ai_for_dummy/</guid>
      <pubDate>Tue, 15 Oct 2024 19:49:08 GMT</pubDate>
    </item>
    <item>
      <title>Podcastfy AI：一款免费开源工具，可将任何内容转换为 AI 生成的音频对话（周末项目）</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4gdp8/podcastfy_ai_a_free_opensource_tool_that_turns/</link>
      <description><![CDATA[🚀 我很高兴发布 Podcastfy.ai：一个开源 Python 包和 CLI 工具，可使用 GenAI 将多模式内容转换为引人入胜的多语言音频对话；类似于 Google 的 NotebookLM，但开放、可编程且可定制。您只需“pip install podcastfy”即可立即开始使用它！ 您可以在论文、简历、网站甚至艺术品图像上运行它，以及上述组合！ 🌟 我对 Google 最新的 GenAI 产品 NotebookLM 很感兴趣，尤其是它的“深度探索”播客功能，可将上传的内容转换为双人 AI 生成的音频对话。正如 Andrej Karpathy 所说，“NotebookLM [...] 是对使用 LLM 的用户体验的重新构想”我确实同意！ 🤔 然而，在探索 NotebookLM 时，我对它的 UI 感到有点沮丧，这增加了流程的摩擦，让我渴望更多的自动化和定制选项。这引发了一个问题：我们能否将 NotebookLM 播客功能的精髓复制为可定制的 API？ 💡 为了解决这个问题，我开发了 Podcastfy – 一个使用 Cursor dot com 构建的周末项目 – 类似于 NotebookLM 的播客功能，但开放、编程且可由任何人定制。 🔑 主要特点： - 从多个来源（例如 URL、YouTube 和 PDF）和模态（图像+文本）生成对话内容 - 自定义成绩单和音频生成（例如样式、语言、结构、长度） - 为全球内容创建提供多语言支持 🔬 技术亮点： - 灵活的 LLM 与 LangChain 集成，支持基于云的模型和本地模型 - 支持高级文本转语音模型（OpenAI、ElevenLabs 和 Microsoft Edge） - 无缝 CLI 和 Python 包集成以实现自动化工作流程 结论： 虽然 NotebookLM 的 AI 生成声音在质量上仍然无与伦比，但这个项目确实解决了我最初的问题，并展示了当今构建 GenAI 产品的迷人可能性。它现在在 GitHub 上上线，我希望你能查看它甚至做出贡献！ 你今天想 Podcastfy 什么？ 🔗 GitHub：https://github.com/souzatharsis/podcastfy/ OpenSource #GenAI #NotebookLM    提交人    /u/HighlanderNJ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4gdp8/podcastfy_ai_a_free_opensource_tool_that_turns/</guid>
      <pubDate>Tue, 15 Oct 2024 19:39:02 GMT</pubDate>
    </item>
    <item>
      <title>论文表明，即使使用人工智能作为工具，法学硕士的表现也优于医生</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4f1gl/paper_shows_llms_outperform_doctors_even_with_ai/</link>
      <description><![CDATA[由于拥有医学和人工智能背景，我对尝试了解大型语言模型 (LLM) 在现实诊断场景中的表现与医生相比有何不同很感兴趣。考虑到最近有批评指出 LLM 似乎会记住基准数据并夸大其性能指标，我特意寻找未受污染的基准。这意味着模型不可能看到数据，这让我们对 LLM 与医生的比较有一个真实的印象。 一项研究特别引起了我的兴趣：在这项研究中（[2312.00164] Towards Accurate Differential Diagnosis with Large Language Models (arxiv.org)），他们表明 LLM 在现实诊断场景中的表现优于医生即使医生可以使用 LLM 来帮助他们。他们的正确率为 35.4%，而医生（平均有 11 年经验）的正确率仅为 13.8%。此外，他们还表明，他们的前 10 个诊断包含正确诊断的频率远远高于医生（55.4% vs. 34.6%）。当他们让医生访问 LLM 时，他们的表现再次不尽如人意（诊断为 24.6%，前 10 个诊断为 52.3%）。 现在还要考虑，由于使用的模型不具备视觉功能，某些数据（如实验室结果）不会输入到模型中，而医生可以访问这些数据。尽管存在这种差异，但 LLM 的表现仍然优于医生。 仅 LLM 的表现就优于使用 GPT 作为补充的医生，这一事实使人们对 AI 只会成为医生工具的观点产生了质疑。LLM 的表现可能仅受到医生的拖累。他们可能会忽略 LLM 的正确建议，高估自己的能力。 想象一下，你有一个能力较差的实习生，他采纳你的建议并做出最终决定，而不是你利用实习生来做出最终决定。让表现优秀的人来负责是有道理的，否则，只会被表现较差的人所拖累。与其让医生把 LLM 当作工具，不如让 LLM 把医生当作工具，这可能更有意义。想象一个未来 LLM 做出最终决定，而医生只充当模型的补充角色，这并不太牵强。 我制作了一个关于这篇论文的视频，为相关研究增加了额外的深度。    提交人    /u/PianistWinter8293   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4f1gl/paper_shows_llms_outperform_doctors_even_with_ai/</guid>
      <pubDate>Tue, 15 Oct 2024 18:42:29 GMT</pubDate>
    </item>
    <item>
      <title>人工智能 (AI) 正在撰写 NSFW 独白？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4a4j3/ai_writing_nsfw_monologues/</link>
      <description><![CDATA[大家好！ 我正在寻找一种 AI 工具，它可以针对某些主题和从某些角度撰写 nsfw 独白。主要是从女性统治的角度。所以，我也可以用不同的方式做到这一点。这些文本应该可以用来制作 hentai 字幕（为了清除，只需谷歌一下）。到目前为止，我只找到了一些写故事的网站，但重要的是 AI 只能像人一样说话。 （抱歉，我的英语不好，这不是我的母语） 感谢您的帮助，希望大家度过美好的一天。    提交人    /u/Random-mate2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4a4j3/ai_writing_nsfw_monologues/</guid>
      <pubDate>Tue, 15 Oct 2024 15:16:02 GMT</pubDate>
    </item>
    <item>
      <title>每周 AI 更新（10 月 9 日至 10 月 15 日）：来自 Google、Tesla、OpenAI、Adobe、AMD 等的重要新闻</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g4a0e4/weekly_ai_updates_oct_09_to_oct_15_major_news/</link>
      <description><![CDATA[分享一个易于理解的、较小的版本的 AI 世界过去一周的主要更新。  Google 的 Imagen3 创造了照片级逼真的 AI 艺术：Google 的高质量文本转图像模型可以生成细节更丰富、光线更好、干扰更少的图像。它可以捕捉复杂的细微差别，如特定的相机角度和复杂的构图，从而生成高度准确和多样化的图像。 特斯拉推出网络出租车机器人出租车：这款外观未来主义的车辆拥有两个翼状门，没有踏板或方向盘。它旨在在没有司机的情况下载客，预计将于 2026 年投入生产。 OpenAI 发布元提示：提示可以指导语言模型根据任务描述生成提示。它还包括分步说明、示例、注释等部分。 OpenAI 发布了 Swarm 多智能体框架：这个开源框架是一个用于创建、编排和部署多智能体系统的实验性工具。它的重点是使智能体协调高度可控且易于测试。 Adobe 发布了带有新工具的 Firefly 视频模型：该工具提供生成填充、文本转图像、生成扩展和生成视频等功能，允许创意人员通过静止图像和基于文本的提示来扩展素材和生成视频。  还有更多……  Zoom 宣布将允许用户创建自定义头像来录制和向团队发送短信，可能会在明年年初推出。  DeepMind CEO Demis Hassabis 和 DeepMind 资深研究员 John Jumper 因计算人类蛋白质结构的 AlphaFold2 AI 模型获得诺贝尔化学奖。  Rabbit 发布了一款新型 Large Action Model，可以控制整套桌面 Linux 操作系统。 AMD 发布了新款 AI 芯片 MI325X，将于第四季度出货，据传将击败 NVIDIA 的 H200。  Reddit 推出了 AI 关键词定位，具有动态受众扩展、多展示位置优化、AI 关键词建议和统一定位流程等功能。 一家 AI 研究公司分析称，谷歌很可能拥有全球最大的 AI 计算能力，相当于至少 60 万块 Nvidia H100 GPU。  据报道，苹果计划推出功能强大的智能眼镜，与 Meta 的雷朋眼镜一较高下。  据报道，微软 GenAI 研究副总裁 Sebastian Bueback 计划加入 OpenAI，进一步致力于开发 AGI。  谷歌已与 Kairos Power 签署协议，从其小型模块化反应堆购买能源，为其人工智能运营提供动力。  Opera 浏览器发布了由 AI 驱动的选项卡命令，允许用户使用自然语言命令和查询对选项卡进行分组、固定、添加书签和关闭选项卡。  有关这些新闻和创新的更详细分类，请参阅时事通讯。    提交人    /u/RohitAkki   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g4a0e4/weekly_ai_updates_oct_09_to_oct_15_major_news/</guid>
      <pubDate>Tue, 15 Oct 2024 15:11:14 GMT</pubDate>
    </item>
    <item>
      <title>人工智能如何降低政府成本？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g423e4/how_can_ai_reduce_the_cost_of_government/</link>
      <description><![CDATA[我真心希望 AI 能够帮助降低政府成本。感觉世界各地的经济体总是资金紧张，在某个时候，增加税收似乎对任何人都没有好处。 在当今社会，有哪些实际方法可以应用 AI 来真正降低政府成本？很想听听这个社区的想法和意见。    提交人    /u/Shot_Mathematician44   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g423e4/how_can_ai_reduce_the_cost_of_government/</guid>
      <pubDate>Tue, 15 Oct 2024 07:18:51 GMT</pubDate>
    </item>
    <item>
      <title>AI 对抗 Zork：一项出乎意料的艰巨挑战</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1g3xyiy/ai_vs_zork_a_surprisingly_difficult_challenge/</link>
      <description><![CDATA[我有点惊讶。我想到了一个简单的想法：让法学硕士玩一款基于文本的游戏——经典的 Zork。对于那些不熟悉的人来说，Zork 是最早的交互式虚构游戏之一，最初开发于 20 世纪 70 年代末。这是一款基于文本的冒险游戏，玩家输入命令来穿越神秘的地下世界、解决谜题、收集宝藏，并克服诸如巨​​魔或上锁的门之类的障碍。没有图形——只有玩家使用简单的文本命令（例如“向北走”、“检查灯笼”或“攻击巨魔”）与之交互的环境、物体和事件的描述。它被认为是一款经典游戏，但可能相当具有挑战性，因为它需要逻辑思维和仔细探索。 从表面上看，它似乎很容易实现，事实上确实如此。但是，我的具有 8B 参数的 LLaMA 3.1 模型在游戏中很难取得任何有意义的进展。 我使用 OpenAI 的 GPT-4o 迅速重写了该程序，它的表现好多了，尽管这是有代价的——每小时要花费 1 美元。成本本来会更高，但我让应用程序使用 TTS 大声读出输出，这样我就可以跟踪进度，而不需要手动阅读所有内容。 在切换到 GPT-4o 之前，我整个周末都在用 LLaMA 3.1 8B 调整提示，希望至少让它打败地下室里的巨魔。相反，它一直在森林里绕圈走，或者努力打开灯笼。 为了省钱，我决定通过将命令复制并粘贴到 ChatGPT 中来手动玩游戏。当然，GPT-4o 没有问题。我也尝试了 GPT-4o mini，但它遇到了与我的小型 LLaMA 3.1 模型类似的问题。相比之下，完整的 GPT-4o 轻松完成游戏，处理库存，击败巨魔，并毫无困难地探索地牢。 与此同时，Meta 的 LLaMA 3.1 模型（具有 405B 个参数）在森林中迷路了，Gemini 卡在了 examine 命令上。Zork 没有提供太多关于检查物体的信息，Gemini 最终欺骗了我，暗示我的游戏版本可能有错误，或者命令中出现了拼写错误。 Claude 的表现相当不错，但我没有进一步探索，因为我厌倦了无休止地复制粘贴命令。我还尝试了具有 70B 参数的 LLaMA 3.1。由于这款游戏在我的笔记本电脑上运行速度非常慢，我把它设置好后，躺在床上用我的 Quest 3 看了两个小时的 YouTube。当我回来时，我发现模型陷入了一个荒谬的循环，仍在试图打开一扇被钉死的门。 TL;DR：我很惊讶法学硕士们竟然在一个看似简单的游戏中如此挣扎。也许有一天他们终于能摆脱这个喷子——在那之前，我会坚持看 YouTube。    提交人    /u/mika314   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1g3xyiy/ai_vs_zork_a_surprisingly_difficult_challenge/</guid>
      <pubDate>Tue, 15 Oct 2024 02:52:58 GMT</pubDate>
    </item>
    </channel>
</rss>